# Phase 4: Parser Unification - IN PROGRESS ðŸ”„

## Current Status: Semantic Compression Applied, Plugin Architecture Needed

### Completed (Phase 4a): Validation Unification âœ…
- **Lines:** 2208 (from 2011 baseline)
- **Tests:** 50/50 headless, 145/148 fuzzer (98%)
- **Validation:** Single source of truth via validateIdentifier()
- **Commits:** 5 commits (main branch)

**Achieved:**
1. âœ… validateIdentifier(str, context) - Rejects camelCase at primitive level
2. âœ… normalizePathStrict(path, context) - Unified kebabâ†’camel conversion  
3. âœ… tokenizeDirective(attr, start) - Core state machine for directive parsing
4. âœ… Removed all ad-hoc fallbacks from data-dump
5. âœ… Removed redundant data-dump extra sweep (was doubling DOM traversal)

### Critical Discovery: Need Plugin Architecture (Phase 4b)

**Problem:** Each data-attr has **semantic inconsistencies** in how they interpret tokens.

**WAIT - Deeper Analysis of `@` Semantics:**

Let me analyze what `@` actually MEANS across directives:

#### Semantic Analysis of `@` Token:

**data-sub** - `@` = "triggered by" (source of change)
```html
data-sub:.@user.name          â†’ Update element WHEN user.name changes
data-sub:.style.color@user.ui.theme-color â†’ Update style.color WHEN theme-color changes
data-sub:postObjs.0.title@.click â†’ Update signal WHEN element is clicked
```
Pattern: `target@SOURCE` where SOURCE triggers the update

**data-sync** - `@` = "source direction" (one-way binding)
```html
data-sync:user.name           â†’ Two-way (implicit :.@. + @.:)
data-sync:user.name@.         â†’ Element -> Signal (WHEN element changes)
data-sync@user.name           â†’ Signal -> Element (WHEN signal changes)
```
Pattern: `signal@SOURCE` where SOURCE is what triggers sync

**data-dump** - `@` = "data source signal"
```html
data-dump@posts#tpl-post      â†’ Render FROM posts signal using template
data-dump@threads             â†’ Render FROM threads signal (inline template)
```
Pattern: `@SOURCE#template` where SOURCE is the data to iterate

#### Key Insight: `@` Always Means "SOURCE"!

The `@` token is **semantically consistent** across all directives:
- `@` = "from/source/origin/when"
- It always indicates WHERE the data/trigger comes from
- It's the **reactive dependency** - what causes this directive to activate

**Why data-dump uses `@signal` NOT `:target`:**

In data-dump, there IS no explicit `:target` because:
1. **Target is implicit:** The element itself (its children get populated)
2. **Source is explicit:** `@posts` - the signal to watch and iterate over
3. **Template is explicit:** `#tpl-post` - what to clone per item

So `data-dump@posts#tpl` is actually:
```
data-dump[implicit :.]@posts#tpl
          ^^^^^^^^  ^^^^^^  ^^^^
          target    source  template
          (this el) (data)  (clone)
```

This is CORRECT and consistent with the grammar!

#### Corrected Understanding:

| Directive | `:target` | `@source` | `#template` | Semantics |
|-----------|-----------|-----------|-------------|-----------|
| data-sub | What to update | What triggers update | Element ref (optional) | `target@SOURCE` |
| data-sync | Signal to sync | Which side triggers | N/A | `signal@DIRECTION` |
| data-dump | **Implicit `.`** | Data signal | Template to clone | `@SOURCE#template` |
| data-def | Signal(s) to define | N/A | N/A | `:signals` |
| data-action | Where to store result | Event trigger | Element ref (optional) | `target@trigger` |

**Semantic Model is Actually CONSISTENT!**
- `:` = target/destination/what
- `@` = source/trigger/when/from
- `#` = element/template reference

**The "inconsistency" isn't in grammar - it's in REQUIRED vs OPTIONAL tokens:**

**The "inconsistency" isn't in grammar - it's in REQUIRED vs OPTIONAL tokens:**

| Directive | Required Tokens | Optional Tokens | Implicit Tokens |
|-----------|----------------|-----------------|-----------------|
| data-sub | `:target`, `@source` | `#elem`, `__mod` | None |
| data-sync | `:signal` | `@direction` | `@.` (two-way) if no @ |
| data-dump | `@source`, `#template` | None | `:. (target)` |
| data-def | `:signal` OR value | value OR `:signal` | None |
| data-action | `:target`, `@trigger` | `#elem`, `__mod`, `^header`, `+input`, `?state` | None |

**Revised Phase 4b Goal:** Not grammar unification (it's already consistent!), but:

### Phase 4b Goal: Token Requirement Specification System

The grammar IS unified. The issue is: **Each attr has different requirements**.

**Core Principle:** Tokenizer parses ALL tokens. Each attr specifies what it needs.

```javascript
const DIRECTIVE_SPECS = {
  'data-sub': {
    requires: {targets: '1+', triggers: '1+'},
    optional: {elementRefs: true, modifiers: true},
    implicit: {},
    value: {required: true, type: 'expression'}
  },
  
  'data-sync': {
    requires: {targets: '1'},  // :signal
    optional: {triggers: '1'}, // @. or @signal
    implicit: {trigger: '.' },  // If no @, assume @.
    value: {required: false, type: 'expression'}
  },
  
  'data-dump': {
    requires: {triggers: '1', templateRef: '1'}, // @signal #template
    optional: {},
    implicit: {target: '.'},  // Target is always current element
    value: {required: false}  // No value needed (for now)
  },
  
  'data-def': {
    requires: {targets: '0+', value: true}, // Either :signals OR value or both
    optional: {modifiers: true},  // REMOVE arbitrary rejection!
    implicit: {},
    value: {required: true, type: 'json|expression'}
  }
};
```

**Key insight:** The inconsistency isn't SEMANTIC, it's SPECIFICATION.
- Grammar is unified âœ…
- Validation is unified âœ…  
- **Specification is ad-hoc** âŒ (each attr hardcodes requirements)

### Phase 4b Tasks (REVISED):

1. âœ… **Analyze `@` semantics** - DONE! It's consistent (SOURCE)
2. **Create DIRECTIVE_SPECS table** - Document token requirements per attr
3. **Build spec validator** - Generic function that checks tokens against spec
4. **Refactor setup functions** - Use spec validator instead of ad-hoc checks
5. **Remove arbitrary restrictions** - data-def should accept modifiers (per spec)
6. **Document implicit tokens** - data-sync:foo implies @., data-dump implies :.
7. **Unify error messages** - "data-X requires :target" vs ad-hoc errors

**Step 5:** Create directive plugin specification
```javascript
// Each data-attr exports a specification:
const DATA_SUB_SPEC = {
  name: 'data-sub',
  prefix: 'data-sub',
  requires: {
    targets: '1+',      // At least one :target
    triggers: '1+',     // At least one @trigger
    value: 'expression' // Attribute value is JS expression
  },
  supports: {
    elementRefs: true,  // Can use #elemId in targets
    modifiers: true,    // Can use __mod
    propPaths: true     // Can use .prop.path in targets
  },
  interpret: (tokens, value) => {
    // Domain-specific logic
    // Returns: handler setup or error
  }
};
```

**Step 6:** Refactor setup functions to use specs
- Remove ad-hoc validation (let tokenizer handle)
- Use spec to validate token requirements
- Focus setup logic on domain semantics only

### Success Criteria (Phase 4b):
- [ ] All attrs use tokenizeDirective (no custom parsers)
- [ ] Clear plugin specification for each attr
- [ ] Documented grammar: what each token means per attr
- [ ] Attribute VALUE semantics documented
- [ ] No arbitrary restrictions (e.g., data-def rejects mods)
- [ ] Tests: 100% pass rate maintained
- [ ] Size: Net reduction (remove duplicate logic)

---

## Completed Work (Phase 4a)

### Architecture Created
1. **validateIdentifier(str, context)** - Rejects camelCase at primitive level
2. **normalizePathStrict(path, context)** - Unified kebabâ†’camel conversion  
3. **tokenizeDirective(attr, start)** - Core state machine for directive parsing

### Parser Status
- âœ… **parseDataAttrFast:** Thin wrapper over tokenizeDirective
- âœ… **parseActionAttr:** Uses tokenizer + validateIdentifier for headers/inputs/state
- âœ… **setupDump:** Custom scan() with validateIdentifier (NEEDS Phase 4b redesign)
- âœ… **setupDef:** Uses tokenizer, rejects modifiers (ARBITRARY - remove in Phase 4b)
- âœ… **setupSync:** Uses parseDataAttrFast BUT has implicit trigger semantics (NEEDS docs)

### Cleanup Completed (Jan 5, 2026)
- âœ… Removed data-dump scan(attrVal) - was checking attribute value
- âœ… Removed data-signal fallback - undocumented, never used
- âœ… Removed attrVal bare signal name - never used
- âœ… Removed element attribute scanning - over-engineered
- âœ… Removed redundant data-dump extra sweep - was doubling DOM traversal
- âœ… Fixed setupDump signature: (el, attr) - consistent with setupSync
- âœ… Enhanced fuzzer: +12 data-dump tests (3 â†’ 15 total)

### Tests Results
- **Headless:** 50/50 (100%) âœ…
- **Fuzzer:** 145/148 (98%) - 3 false negatives due to JSDOM timing
- **Actions:** All pass âœ…

---

## Original Implementation Plan (Phase 4a - COMPLETE)

## Current State Analysis

### Parser Locations
1. **parseDataAttrFast** (lines 551-640, ~90 lines)
   - Most complete: handles all token types
   - Has validation via normalizePath()
   - State machine: scans `:` and `@` delimiters

2. **parseActionAttr** (lines 989-1072, ~83 lines)  
   - Custom logic for `^` headers, `+` inputs, `?` state
   - Calls parseDataAttrFast for `:target@trigger` part
   - Manual validation added for inputs

3. **parseDumpAttr** (lines 1477-1487, ~11 lines)
   - Simple indexOf scan for `@signal` and `#template`
   - Regex validation (allows camelCase)

4. **setupDef** (lines 1879-1892, ~14 lines)
   - substring after `:`
   - No validation

### Shared Grammar Elements
All parsers handle:
- **Delimiters:** `:` (target), `@` (trigger), `#` (element), `.` (prop/current), `_` (special)
- **Modifiers:** `__modifier` or `__modifier.value`
- **Identifiers:** kebab-case names (should reject camelCase)
- **Paths:** dot-notation like `user.ui.theme-color`

### Action-Specific Tokens
- `^` headers: `^json`, `^auth.Bearer`
- `+` inputs: `+#id.prop`, `+signal`
- `?` state: `?busy,err`
- Body modifiers: `__body.path`, `__query.name`, `__header.name`

## Implementation Strategy

### Step 1: Extract Core Tokenizer (NEW)
Create `tokenizeDirective(str, start, stopChars)` that:
- Returns: `{targets: [], triggers: [], globalMods: {}, extras: {}}`
- Scans `:` and `@` delimiters
- Delegates to helper functions for each token type
- Uses shared `normalizePath()` for all identifiers
- Handles global modifiers (`__` at start)

### Step 2: Extract Validation Helpers
Move validation to primitive level:
- `validateIdentifier(str, context)` - rejects camelCase
- `normalizePathStrict(str, context)` - validates + converts kebabâ†’camel
- Called by tokenizer, not by each parser

### Step 3: Rewrite parseDataAttrFast
Simply call tokenizer:
```javascript
function parseDataAttrFast(attr, prefixLen) {
  return tokenizeDirective(attr, prefixLen, null);
}
```

### Step 4: Rewrite parseActionAttr  
Use tokenizer + interpret action tokens:
```javascript
function parseActionAttr(attr) {
  // Extract method
  // Parse action-specific tokens (^+?)
  // Call tokenizer for :target@trigger part
  // Return unified format
}
```

### Step 5: Rewrite parseDumpAttr
Use tokenizer:
```javascript
function parseDumpAttr(name, val) {
  const tokens = tokenizeDirective(name + '@' + val, 0, null);
  return {
    sig: tokens.triggers[0]?.name,
    tplId: tokens.extras.templateId
  };
}
```

### Step 6: Update setupDef
Add validation:
```javascript
function setupDef(el, attr, value) {
  const colonIdx = attr.indexOf(':');
  const name = colonIdx === -1 ? '' : attr.substring(colonIdx + 1);
  if (!validateIdentifier(name, attr)) return;
  // ... rest of logic
}
```

## Testing Strategy

1. Run fuzzer after each step - must stay 100%
2. Run headless tests - must stay 50/50
3. Run action tests - must pass
4. Add tokenizer-specific tests
5. Test edge cases: empty strings, malformed syntax, camelCase rejection

## Success Criteria

- âœ… All tests pass (fuzzer 100%, headless 50/50, actions pass)
- âœ… Size reduction: -100 to -150 lines (semantic compression)
- âœ… Single source of validation logic
- âœ… Each parser is thin wrapper over tokenizer
- âœ… Adding new syntax requires one change (tokenizer only)
- âœ… No validation gaps possible (all go through same code path)

## Rollout

1. Implement on dev-phase4-parser-unification branch
2. Commit after each step with tests passing
3. Final commit: measure size reduction
4. Merge to main after review
5. This unlocks Pattern 1-10 work (safe foundation)
